{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an open-source, commercially usable machine learning toolki: [scikit-learn](https://scikit-learn.org/stable/index.html). This toolkit contains implementations of many of the algorithms seen in other notebooks of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a helper function to load the house data features and prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_dataset():\n",
    "    print(os.getcwd)\n",
    "    '''\n",
    "    Loads both features and targer values from the raw dataset, as specified in the path below.\n",
    "    Example of data in the file:\n",
    "\n",
    "    [feature_x1, feature_x2, feature_x4, feature_x5, target_y]\n",
    "    1.244000000000000000e+03,3.000000000000000000e+00,1.000000000000000000e+00,6.400000000000000000e+01,3.000000000000000000e+02\n",
    "    1.947000000000000000e+03,3.000000000000000000e+00,2.000000000000000000e+00,1.700000000000000000e+01,5.098000000000000114e+02\n",
    "\n",
    "        Returns: X matrix and y vector, numpy structures.\n",
    "\n",
    "    '''\n",
    "    data = np.loadtxt(\"../data/raw/houses_features_prices.txt\", delimiter=',', skiprows=1)\n",
    "    \"\"\"\n",
    "    This line selects all rows (indicated by the : before the first comma) and all columns except the last one (indicated by :-1). \n",
    "    The :-1 means \"from the beginning to the second last element\" (or \"from the beginning to the last element minus one\"). \n",
    "    \n",
    "    This creates a new numpy array X containing all the features (i.e., all columns except the last one).\n",
    "    \"\"\"\n",
    "    X = data[:, :4]\n",
    "    \"\"\"\n",
    "    This line selects all rows (indicated by the : before the comma) and only the last column (indicated by -1). \n",
    "    In numpy, -1 refers to the last index. This creates a new numpy array y containing only the target variable (i.e., the last column).\n",
    "    \"\"\"\n",
    "    y = data[:, 4]\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Scikit-Learn\n",
    "Scikit-learn has a gradient descent regression model [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#examples-using-sklearn-linear-model-sgdregressor).  Like previous implementations of gradient descent in this project, the model performs best with normalized inputs. [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) will perform z-score normalization . In the library it is referred to as 'standard score'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code loads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function getcwd>\n",
      "First row loaded features: [1.244e+03 3.000e+00 1.000e+00 6.400e+01] - Target: 300.0\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_house_dataset()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "print(f\"First row loaded features: {X_train[0]} - Target: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to normalize the training data, that is, scale it to allow for Gradient Descent to run much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:[2.406e+03 4.000e+00 1.000e+00 9.500e+01]\n",
      "Peak to Peak range by column in Normalized X:[5.8452591  6.13529646 2.05626214 3.68533012]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create and fit the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor()\n",
      "number of iterations completed: 117, number of weight updates: 11584.0\n"
     ]
    }
   ],
   "source": [
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the obtained parameters now, which should be associated with the normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:                   w: [110.05889707 -21.01156267 -32.42551666 -38.06763388], b:[363.15290664]\n"
     ]
    }
   ],
   "source": [
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Now that the library algorithm provided the parameters and bias, we can make predictions with them, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction using np.dot() and sgdr.predict match: True\n",
      "Prediction on training set:\n",
      "[295.17713373 485.87932857 389.6014757  492.04143246]\n",
      "Target values \n",
      "[300.  509.8 394.  540. ]\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "# The prediction can also be done 'manually', that is, using the b and w obtained before and numpy dot product\n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm\n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
